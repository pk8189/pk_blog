import styles from './document.module.css'

<div className={styles["Welcome"]}>

## Filtering lists in python

A common task for a python engineer is to filter data based off of some criteria.
The engineer may be pulling data from a 3rd party resource to perform some analysis, but
the third party API does not allow for a query specific enough to fit their needs.
Therefore, the return value of the API must be filitered before moving the data any further.
Often times, the data that needs to be filtered comes in the form of a list. 
To filter lists in standard python, 4 options come to mind:
 
 1) [Populate a new list with a for loop containing a conditional statement](#Standard-for-loop-with-a-conditional)

 2) [Use list comprehension](#List-comprehension)
 
 3) [Create a new list with filter and lambda function](#Filter-+-Lambda)
 
 4) [Generator functions](#The-generator)

It's important for the engineer to be understand the pros 
and cons of each filtration method processing the incoming data. It's also important
to note that the output type of the result `type(filtered_data)` of Filtration methods 1-3
is `<class 'list'>`, but type 4 is `<class 'generator'>`. We will cover the implications of this in 
[the generator section](#The-generator).

## Generating the fake data
In order to evaluate the performance of our four filtration methods, we need some data.
Luckily it's quite easy to create and save fake data in standard python. We will use the `random` module to 
generate random numbers to populate unfiltered lists, and use the `pickle` module to save the data.
We use `random.randint(lowerBound, upperBound)` to generate random integers between 0 and 100000.
We use `pickle.dump(mockData)` to save our generated list to a file. This way we don't have to generate the 
mock data every time we want to test our filtration methods.  

```python
from random import randint
import pickle

with open('one_hundred_thousand_data_points.pkl', 'wb') as f:
    pickle.dump([randint(0, 100000) for _ in range(100000)], f)

with open('one_million_data_points.pkl', 'wb') as f:
    pickle.dump([randint(0, 100000) for _ in range(1000000)], f)

with open('ten_million_data_points.pkl', 'wb') as f:
    pickle.dump([randint(0, 100000) for _ in range(10000000)], f)

with open('one_hundred_million_data_points.pkl', 'wb') as f:
    pickle.dump([randint(0, 100000) for _ in range(100000000)], f)
```

Now that our fake data has been generated by running the above script, we can test our filtration methods with 
another script which starts by loading the .pkl files to run through each method. Each filtration
method is written as a function which takes a list as an argument.

## Standard for loop with a conditional
The most straightforward way to filter a list is to loop through the unfiltered list with a 
for loop, appending all data points which meet a conditional into a new list, which we return
once it has processed every data point. We will see in the following section that list comprehension is
both faster and more readable than the below code block.

```python
def conditional_loop(data):
    results = []
    for item in data:
        if item < 50000:
            results.append(item)
    return results
```

## List comprehension
Using list comprehension is a nice pythonic way to create a new list of filtered data.
This method allows us to perform the filtration in one readable line of code, and is faster than a regular
for loop.  The reason that it performs better is because it doesn't need to load the `append` attribute 
of the list and call it as a function at each iteration. Suspending and resuming a function's frame, or 
multiple functions in other cases, is slower than creating a list on demand.
```python
return [item for item in data if item < 50000]
```
## Filter + Lambda
The `filter()` method filters the given sequence with the help of a function that tests each element 
in the sequence to be true or not. Using `lambda` allows us to define the function in the same line as the filter.
Our lambda simply returns a boolean which depends on if the data point fits the condition.
We have to wrap the return with `list()` because the type of filter(lambda) is `<class 'filter'>` which is not iterable.
This filtration method is not great in terms of readability, and it showed the worst performace (by far) according to 
our metrics when running on the pickled data. I have not found a use case for filter personally, but could see a scenario
where an engineer may want to keep the original list intact with a filter applied. Our other 3 methods do not form this 
connection because brand new structures are being created.


```python
return list(filter(lambda x: x < 50000, data))
```

## The generator
Generator functions return a lazy iterator. Lazy iterators are objects that you can loop over 
like a `<class 'list'>`. 
However, unlike lists, lazy iterators do not store their contents in memory.
Lazy evaluation is useful when you have a very large data set to compute. It allows you to start 
using the data immediately, while the whole data set is being computed.
Iterators and generators can only be iterated over once.


```
def generator_create(data):
    for item in data:
       if item < 50000: yield item
```

## Another improvement: Generator Expressions.
This is the list comprehension equivalent of generators. 
It works exactly in the same way as a list comprehension, but the `()` instead of `[]` wrapping 
the code makes it a generator.
For our use case in the beginning of this article, this would be the winner.

```
return (item for item in data if item < 5000)
```

## Extra info: using function decorators for benchmarking. 
A decorator is a function that takes another function and extends the behavior of
the latter function without explicitly modifying it.
Python allows you to use decorators in a simpler way with the @ symbol, 
sometimes called the “pie” syntax.
Our @benchmarks decorater assumes it's input function takes in 1 arg and 1 kwarg called size.
This allows us to wrap each of our filtrations functions in @benchmarks and save valuable lines of code.

```
def benchmarks(func):
    @functools.wraps(func)
    def time_it(data, **kwargs):
        start_time = time()
        value = func(data)
        elapsed_time = time() - start_time
        print(f"{func.__name__} filtered {kwargs['size']} data points in: {elapsed_time}")
        return value
    return time_it
```

#### Here is the anaylsis script for reference:


```python
import functools
from time import time
import pickle
from pathlib import Path

def benchmarks(func):
    @functools.wraps(func)
    def time_it(data, **kwargs):
        start_time = time()
        value = func(data)
        elapsed_time = time() - start_time
        print(f"{func.__name__} filtered {kwargs['size']} data points in: {elapsed_time} seconds")
        return value
    return time_it

@benchmarks
def conditional_loop(data):
    results = []
    for item in data:
        if item < 50000:
            results.append(item)
    return results

@benchmarks
def list_comprehension(data):
    return [item for item in data if item < 50000]

@benchmarks
def filter_lambda(data):
    return list(filter(lambda x: x < 50000, data))

@benchmarks
def generator_create(data):
    for item in data:
       if item < 50000: yield item

@benchmarks
def generator_exp(data):
    return (item for item in data if item < 5000)

if __name__ == "__main__":
    all_functions = [
        conditional_loop,
        list_comprehension,
        filter_lambda,
        generator_create,
        generator_exp,
    ]
    data_path = Path(__file__).resolve().parent / "mock_data"
    for pkl in data_path.glob("*.pkl"):
        with open(pkl, "rb") as f:
            data = pickle.load(f)
            size = len(data)
            for func in all_functions:
                print("--------------")
                filtered_data = func(data, size=size)
            print("-----------------------------------------")

```

#### And the results output:
```
conditional_loop filtered 100,000,000 data points in: 6.975962162017822 seconds
--------------
list_comprehension filtered 100,000,000 data points in: 4.718896865844727 seconds
--------------
filter_lambda filtered 100,000,000 data points in: 8.073183059692383 seconds
--------------
generator_create filtered 100,000,000 data points in: 1.0967254638671875e-05 seconds
--------------
generator_exp filtered 100000000 data points in: 3.0994415283203125e-06 seconds
-----------------------------------------
conditional_loop filtered 10,000,000 data points in: 0.5472168922424316 seconds
--------------
list_comprehension filtered 10,000,000 data points in: 0.35300588607788086 seconds
--------------
filter_lambda filtered 10,000,000 data points in: 0.7780559062957764 seconds
--------------
generator_create filtered 10,000,000 data points in: 9.5367431640625e-07 seconds
--------------
generator_exp filtered 10,000,000 data points in: 3.0994415283203125e-06 seconds
-----------------------------------------
conditional_loop filtered 1,000,000 data points in: 0.054818153381347656 seconds
--------------
list_comprehension filtered 1,000,000 data points in: 0.03512120246887207 seconds
--------------
filter_lambda filtered 1,000,000 data points in: 0.07900500297546387 seconds
--------------
generator_create filtered 1,000,000 data points in: 9.5367431640625e-07 seconds
--------------
generator_exp filtered 1,000,000 data points in: 3.0994415283203125e-06 seconds
-----------------------------------------
conditional_loop filtered 100,000 data points in: 0.005515098571777344 seconds
--------------
list_comprehension filtered 100,000 data points in: 0.0036470890045166016 seconds
--------------
filter_lambda filtered 100,000 data points in: 0.00793600082397461 seconds
--------------
generator_create filtered 100,000 data points in: 1.1920928955078125e-06 seconds
--------------
generator_exp filtered 100,000 data points in: 2.1457672119140625e-06 seconds
-----------------------------------------

```
Note: 1.0967254638671875e-05 = 0.000010967254638671875




</div>